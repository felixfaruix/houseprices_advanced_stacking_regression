{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae42cb4",
   "metadata": {},
   "source": [
    "# House Prices - Advanced Regression Techniques (Detailed Project Documentation)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a77d04",
   "metadata": {},
   "source": [
    "1. [Introduction and Objectives](#introduction)\n",
    "2. [Data Loading and Initial Exploration](#data-loading)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Engineering and Custom Transformers](#feature-engineering)\n",
    "5. [Statistical Outlier Detection and Z-Score Analysis](#outlier-detection)\n",
    "6. [Data Preprocessing Pipeline](#preprocessing)\n",
    "7. [Model Implementation and Baseline](#modeling)\n",
    "8. [Automated Hyperparameter Optimization with Optuna](#optimization)\n",
    "9. [Model Evaluation with Cross-Validation and Ensemble Analysis](#evaluation)\n",
    "10. [Predictions and Kaggle Submission](#submission)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements a production-ready machine learning pipeline for predicting house prices using the Kaggle House Prices dataset. The approach emphasizes automated optimization, ensemble learning, and robust evaluation practices suitable for real-world deployment. Involves statistical analysis, data preprocessing, feature engineering, costumized transformers/pipelines and stacking regression. Finally: model evaluation and selection via Optuna Hyperparameter Tuning\n",
    "\n",
    "\n",
    "### Technical Implementation Strategy\n",
    "\n",
    "The solution employs automated ensemble discovery where Optuna simultaneously optimizes model selection and hyperparameters across 9 different algorithm types. This eliminates manual model tuning while ensuring robust cross-validation evaluation. The final stacking architecture combines diverse base learners (linear, tree-based, kernel, instance-based) using a meta-learner that optimally weights predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66a1e6",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "This project tackles house price prediction through ensemble learning pipeline that automates the entire model selection and optimization process.\n",
    "\n",
    "**Custom Feature Engineering Pipeline**\n",
    "- Built 8 custom sklearn transformers handling log transformations, age calculations, outlier removal, categorical encoding, and correlation filtering\n",
    "- Created binary indicators for sparse features (like ScreenPorch, MasVnrArea) to capture zero/non-zero patterns\n",
    "- Combined basement and ground floor areas into unified TotalSF feature\n",
    "- Weighted bathroom counting (full baths = 1.0, half baths = 0.5) for better representation\n",
    "- One-hot encoded categorical features with high cardinality (inspected manually) and non-ordinal. \n",
    "- Ordinal encoded categorical features with low cardinality (inspected manually).\n",
    "\n",
    "**Outlier Detection System**\n",
    "- Implemented z-score statistical analysis across all numerical features\n",
    "- Generated comprehensive outlier tables ranking extreme values by magnitude\n",
    "- Applied systematic outlier removal (|z| > 3.0) only on training data to prevent data leakage\n",
    "- Preserved outliers in validation/test sets for realistic performance evaluation\n",
    "\n",
    "**Automated Model Discovery with Optuna**\n",
    "- 35/50 trial optimization discovering optimal ensemble compositions automatically\n",
    "- Simultaneous hyperparameter tuning for 9 different algorithm types (Linear, Ridge, Lasso, ElasticNet, CatBoost, SVR, KNN, RandomForest, LightGBM, XGBoost)\n",
    "- Dynamic model selection flags allowing Optuna to choose which algorithms to include\n",
    "- Cross-validation scoring ensuring robust model evaluation\n",
    "\n",
    "**Multi-Algorithm Stacking Architecture**\n",
    "- Final ensemble combines 5 diverse base learners: Lasso, CatBoost, SVR, RandomForest, XGBoost\n",
    "- Ridge regression meta-learner optimally weights base model predictions\n",
    "- 5-fold cross-validation prevents overfitting during stacking\n",
    "- Leverages different algorithm strengths: linear relationships (Lasso), non-linear patterns (trees), local patterns (SVR)\n",
    "\n",
    "**Data Pipeline**\n",
    "- Separate pipelines for train (with outlier removal) vs validation/test (without outlier removal)\n",
    "- Consistent feature engineering across all datasets using fitted transformers\n",
    "- DataFrame alignment\n",
    "- Maintained pipeline order preventing any form of data leakage\n",
    "\n",
    "**Model Evaluation**\n",
    "- RMSE, RÂ², and MAE metrics for complete performance assessment\n",
    "- Residual analysis \n",
    "- Features importance analysis using SHAP values\n",
    "- Optuna visualization tools showing optimization history and parameter importance\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79567aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries and type hints for enhanced code clarity\n",
    "from typing import List, Optional, Union, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for data manipulation, visualization, and machine learning\n",
    "import pandas as pd\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading training and test datasets from Kaggle competition\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "train_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35106c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring dataset structure by examining all available features\n",
    "sorted_columns = sorted(train_data.columns)\n",
    "for column in sorted_columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cff3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train-validation split for model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_data.drop(columns=['SalePrice'], axis=1).copy()\n",
    "y = train_data['SalePrice'].copy()\n",
    "\n",
    "# Splitting data with 80-20 ratio, maintaining reproducibility\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reconstructing training data for pipeline processing\n",
    "train_data = X_train.join(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing target variable distribution to understand data characteristics\n",
    "sns.histplot(train_data['SalePrice'], bins=100, kde = True, linewidth=0)\n",
    "plt.xlabel('Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting numerical features for comprehensive analysis\n",
    "numerical_values = train_data.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ca2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating comprehensive histogram visualization for all numerical features\n",
    "numerical_values.hist(figsize=(15, 10), bins=50, xlabelsize=8, ylabelsize=8)\n",
    "for ax in plt.gcf().axes:\n",
    "    ax.set_title(ax.get_title(), fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d48336",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_values.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514efcc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "317a1e5d",
   "metadata": {},
   "source": [
    "## Feature Engineering and Custom Transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d65df",
   "metadata": {},
   "source": [
    "### Log Transformation Strategy\n",
    "\n",
    "Many numerical features exhibit significant right-skewness, particularly area-based measurements like LotArea, GrLivArea, and TotalBsmtSF. Applying log1p transformation reduces skewness and creates more Gaussian-like distributions, which enhances model performance for algorithms that assume normality.\n",
    "\n",
    "The transformation also helps stabilize variance across different scales, making the data more suitable for linear models and improving convergence in gradient-based optimization algorithms. \n",
    "We apply the log transformation only on those features that are appear from continous from the manual investigation. Those features were selected one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformers implementing sklearn interface for systematic preprocessing\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies logarithmic transformation to reduce skewness in numerical features.\n",
    "    Creates binary indicators for sparse features with high zero proportions.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_prop = 0.05, columns =[\"LotArea\", \"GrLivArea\",\n",
    "                                \"TotalBsmtSF\", \"1stFlrSF\", \"LotFrontage\",\n",
    "                                \"GarageArea\", \"ScreenPorch\",\n",
    "                                \"BsmtUnfSF\", \"MasVnrArea\", \"BsmtFinSF1\",\n",
    "                                \"BsmtFinSF2\", \"OpenPorchSF\"]):\n",
    "        self.min_prop = min_prop\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identifying sparse columns for binary indicator creation\n",
    "        self.sparse_columns = []\n",
    "        for col in self.columns:\n",
    "            value_counts = X[col].value_counts(normalize=True)\n",
    "            zero_prop = value_counts.get(0, 0.0)\n",
    "            if zero_prop >= self.min_prop: \n",
    "                self.sparse_columns.append(col)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                X[col] = np.log1p(X[col])\n",
    "                if col in self.sparse_columns:\n",
    "                    X[f\"has_{col}\"] = (X[col] > 0).astype(int)\n",
    "        return X\n",
    "\n",
    "class dropColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Removes specified columns that provide minimal predictive value or cause issues.\"\"\"\n",
    "    \n",
    "    def __init__(self, columns=[]):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            if col in X.columns:\n",
    "                X.drop(columns=col, inplace=True)\n",
    "        return X\n",
    "\n",
    "class AgeCalculator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Computes house age from year sold and year built, providing temporal context.\"\"\"\n",
    "    \n",
    "    def __init__(self, columns=[\"YrSold\", \"YearBuilt\"]):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"HouseAge\"] = (X[self.columns[0]] - X[self.columns[1]]).astype(int)\n",
    "        X[\"HouseAge\"] = X[\"HouseAge\"].clip(lower=0)\n",
    "        return X\n",
    "\n",
    "class FeatureImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handles missing values with domain-specific strategies for different feature types.\"\"\"\n",
    "    \n",
    "    def __init__(self, columns_zeros=[], columns_none = []):\n",
    "        self.columns_zeros = columns_zeros\n",
    "        self.columns_none = columns_none\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.columns_zeros:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].fillna(0)\n",
    "        for col in self.columns_none:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].fillna(\"None\")\n",
    "        return X\n",
    "\n",
    "class OutliersRemoval(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Removes predetermined outlier observations based on statistical analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, rows_indexes =[1182, 1298, 1169, 224, 1190]):\n",
    "        self.rows_indexes = rows_indexes \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for index in self.rows_indexes:\n",
    "            if index in X.index:\n",
    "                X = X.drop(index=index, errors='ignore')\n",
    "        return X.reset_index(drop=True)\n",
    "    \n",
    "class YearImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Imputes missing garage year built values while creating missingness indicators.\"\"\"\n",
    "    \n",
    "    def __init__(self, column=\"GarageYrBlt\", fill_value= None):\n",
    "        self.column = column\n",
    "        self.fill_value = fill_value\n",
    "       \n",
    "    def fit(self, X, y=None):\n",
    "        if self.fill_value is None:\n",
    "            self.fill_value = X[self.column].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"GarageYrBlt_missing\"] = (X[self.column]).isna().astype(int)\n",
    "        X[self.column] = X[self.column].fillna(self.fill_value)\n",
    "        return X\n",
    "\n",
    "class HighlyCorrelatedFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Identifies and removes highly correlated features to reduce multicollinearity.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.82):\n",
    "        self.threshold = threshold\n",
    "        self.columns_to_drop = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        corr_matrix = X.corr().abs()\n",
    "        mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        upper = corr_matrix.where(mask)\n",
    "        self.to_drop_ = [col for col in upper.columns if any(upper[col] > self.threshold)]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febab9f",
   "metadata": {},
   "source": [
    "## Outlier Detection and Analysis\n",
    "\n",
    "### Statistical outlier detection using z-score\n",
    "\n",
    "Outliers can significantly impact model performance, particularly for linear algorithms. The z-score method provides a standardized approach to identify observations that deviate substantially from the dataset mean.\n",
    "\n",
    "The z-score transformation standardizes values by measuring how many standard deviations an observation lies from the mean:\n",
    "\n",
    "$$z_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_{i,j}$ is the sample mean\n",
    "- $\\sigma_j = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_{i,j} - \\mu_j)^2}$ is the sample standard deviation\n",
    "\n",
    "Observations with $|z_{i,j}| > 3$ are typically considered outliers and may warrant removal or special treatment. In this context, we apply z-score analysis to all numerical features, but we only remove outliers from the training set to prevent data leakage. The validation and test sets retain all observations, including outliers, to ensure realistic performance evaluation. Not every outlier needs to be removed, some of them could contain valuable information, especially in real estate where extreme values can indicate unique properties or market conditions. For this reason, we kept msot of the outliers and manually investigate which one to remove later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(numerical_values, title=\"House Prices Regression Data Profiling Report\", explorative=True)\n",
    "profile.to_file(\"house_prices_regression.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4e24b",
   "metadata": {},
   "source": [
    "In the html file `house_prices_regression.html`, we can find the data profiling report which includes the Z-score for each value in the specified columns. The report also provides insights into the distribution of the data, missing values, and other statistical properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a77b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore \n",
    "\n",
    "def get_outlier_table(numerical_values: pd.DataFrame, thresh: float = 3.0):\n",
    "    \"\"\"Creates a comprehensive outlier analysis table using Z-score methodology.\n",
    "    Returns sorted table of extreme values for systematic outlier identification.\"\"\"\n",
    "    \n",
    "    # Computing Z-scores for all numerical features\n",
    "    zscores  = pd.DataFrame(zscore(numerical_values, nan_policy=\"omit\"), \n",
    "                            columns=numerical_values.columns, \n",
    "                            index=numerical_values.index)\n",
    "\n",
    "    # Reshaping to long format for analysis\n",
    "    long = (zscores.stack().reset_index().rename(columns={\n",
    "            \"level_0\": \"row_index\",\n",
    "            \"level_1\": \"feature\",\n",
    "            0: \"z_score\"}))\n",
    "\n",
    "    # Filtering extreme values based on threshold\n",
    "    outliers = long[long[\"z_score\"].abs() > thresh].copy()\n",
    "    \n",
    "    # Sorting by magnitude for prioritized outlier review\n",
    "    outliers = outliers.reindex(\n",
    "        outliers[\"z_score\"].abs().sort_values(ascending=False).index\n",
    "    ).reset_index(drop=True)\n",
    "    return outliers\n",
    "\n",
    "# Generating outlier analysis for systematic review\n",
    "outlier_table = get_outlier_table(numerical_values, thresh=3.0)\n",
    "\n",
    "# Examining outliers in key features for decision-making\n",
    "for feature in [\"LotArea\", \"GrLivArea\", \"SalePrice\",\n",
    "                                \"TotalBsmtSF\", \"1stFlrSF\", \"LotFrontage\",\n",
    "                                \"GarageArea\", \"ScreenPorch\",\n",
    "                                \"BsmtUnfSF\", \"MasVnrArea\", \"BsmtFinSF1\",\n",
    "                                \"BsmtFinSF2\", \"OpenPorchSF\"]:\n",
    "    if feature in outlier_table[\"feature\"].values: \n",
    "        print(outlier_table[outlier_table[\"feature\"] == feature].head(10))\n",
    "\n",
    "def get_outliers_list(outlier_table: pd.DataFrame, n: int = 10):\n",
    "    \"\"\"Extracts list of most extreme outlier indices for removal consideration.\"\"\"\n",
    "    return outlier_table.head(n)[\"row_index\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a543c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=\"SalePrice\", y=\"BsmtFinSF1\", data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f03c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.query(\"SalePrice > 500000\")\n",
    "#1182, 1298, 1169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcad628",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=\"YrSold\", y=\"SalePrice\", data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_data.isnull().sum(), columns=['Null Count']).sort_values(by='Null Count', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184680a5",
   "metadata": {},
   "source": [
    "### Categorical Feature Analysis\n",
    "\n",
    "Examining categorical features to determine appropriate encoding strategies based on feature characteristics and relationships with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used this cell to manually inspect the unique values of different columns\n",
    "train_data[\"GarageFinish\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used this cell to manually fill some missing values in the dataset\n",
    "train_data[\"BsmtFinType1\"] = train_data[\"BsmtFinType1\"].fillna(\"Unf\")\n",
    "train_data[\"BsmtFinType2\"] = train_data[\"BsmtFinType2\"].fillna(\"Unf\")\n",
    "train_data[\"Electrical\"] = train_data[\"Electrical\"].fillna(\"SBrkr\")\n",
    "\n",
    "X_val[\"BsmtFinType1\"] = X_val[\"BsmtFinType1\"].fillna(\"Unf\")\n",
    "X_val[\"BsmtFinType2\"] = X_val[\"BsmtFinType2\"].fillna(\"Unf\")\n",
    "X_val[\"Electrical\"] = X_val[\"Electrical\"].fillna(\"SBrkr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af790ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=\"MasVnrArea\", y=\"LotFrontage\", data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"GarageCond\", y=\"SalePrice\", data=train_data, kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Inspecting categorical features and their unique values'''\n",
    "\n",
    "# Get all categorical columns\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a list to store dictionaries with column info\n",
    "column_info = []\n",
    "\n",
    "# Populate the list\n",
    "for col in categorical_columns:\n",
    "    unique_vals = train_data[col].unique()\n",
    "    column_info.append({'Feature': col, 'Unique_Values': list(unique_vals)})\n",
    "\n",
    "# Create DataFrame from the list\n",
    "feature_values_df = pd.DataFrame(column_info)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "# Set pandas display options to show all values without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "feature_values_df.head(5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing features based on encoding requirements\n",
    "\n",
    "# Nominal categorical variables requiring one-hot encoding\n",
    "one_hot_features = [\"SaleType\", \"MiscFeature\", \"PavedDrive\", \"GarageType\", \"Electrical\", \n",
    "                    \"CentralAir\", \"Heating\", \"Foundation\", \"MasVnrType\", \"RoofMatl\", \n",
    "                    \"Exterior2nd\", \"RoofStyle\", \"HouseStyle\", \"BldgType\", \"Exterior1st\", \n",
    "                    \"Condition2\", \"Condition1\", \"Neighborhood\", \"LotConfig\", \"LotShape\", \n",
    "                    \"Street\", \"MSZoning\", \"SaleCondition\"]\n",
    "\n",
    "# Ordinal categorical variables with inherent quality ordering\n",
    "ordinal_features = [\"GarageCond\", \"GarageQual\", \"GarageFinish\", \"FireplaceQu\", \n",
    "                    \"Functional\", \"KitchenQual\", \"HeatingQC\", \"BsmtFinType2\", \"BsmtFinType1\", \n",
    "                    \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"ExterCond\", \"ExterQual\",\n",
    "                    \"LandSlope\", \"Utilities\", \"LandContour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe92d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining ordinal mappings with proper quality hierarchies from worst to best\n",
    "ordinal_features_dict = { \n",
    "    \"GarageCond\":  [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\", \"None\"],\n",
    "    \"GarageQual\":  [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\", \"None\"],\n",
    "    \"FireplaceQu\": [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\", \"None\"],\n",
    "    \"KitchenQual\": [\"None\",\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"HeatingQC\":   [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"BsmtCond\":    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\", \"None\"],\n",
    "    \"BsmtQual\":    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\", \"None\"],\n",
    "    \"ExterCond\":   [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"ExterQual\":   [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"Functional\": [\"None\", \"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"BsmtFinType1\": [\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtFinType2\": [\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtExposure\": [\"None\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"LandSlope\": [\"Gtl\", \"Mod\", \"Sev\"],\n",
    "    \"Utilities\": [\"None\", \"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"LandContour\": [\"Lvl\", \"Bnk\", \"HLS\", \"Low\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering transformers for categorical and derived features\n",
    "\n",
    "class CatEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handles both one-hot and ordinal encoding with proper categorical handling.\n",
    "    Ensures consistent encoding across train/validation/test splits.\"\"\"\n",
    "    \n",
    "    def __init__(self, one_hot_features=None, ordinal_features_levels=None, drop_first=True):\n",
    "        self.one_hot_features = one_hot_features or []\n",
    "        self.ordinal_features_levels = ordinal_features_levels or {}\n",
    "        self.drop_first = drop_first\n",
    "        self.dummy_cols = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy().fillna(\"Missing\")\n",
    "        \n",
    "        # Establishing dummy column structure during fit phase\n",
    "        ohe = pd.get_dummies(X[self.one_hot_features], drop_first=self.drop_first, dtype='uint8')\n",
    "        self.dummy_cols = list(ohe.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy().fillna(\"Missing\")\n",
    "        \n",
    "        # Applying ordinal encoding with predefined hierarchies\n",
    "        for col, order in self.ordinal_features_levels.items():\n",
    "            if col in X.columns:\n",
    "                cat_type = pd.api.types.CategoricalDtype(categories=order, ordered=True)\n",
    "                X[col] = X[col].astype(cat_type).cat.codes\n",
    "\n",
    "        # Creating one-hot encoded features\n",
    "        ohe = pd.get_dummies(X[self.one_hot_features], drop_first=self.drop_first, dtype='uint8')\n",
    "        \n",
    "        # Ensuring consistent column structure across datasets\n",
    "        for col in self.dummy_cols: # type: ignore\n",
    "            if col not in ohe.columns:\n",
    "                ohe[col] = 0\n",
    "        \n",
    "        ohe = ohe[self.dummy_cols]\n",
    "        X = X.drop(columns=self.one_hot_features)\n",
    "        return pd.concat([X, ohe], axis=1)\n",
    "\n",
    "class TotalArea(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Creates comprehensive area feature by combining basement and ground floor areas.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"TotalSF\"] = (X[\"TotalBsmtSF\"] + X[\"GrLivArea\"])\n",
    "        X.drop(columns=[\"TotalBsmtSF\", \"GrLivArea\"], inplace=True, errors='ignore')\n",
    "        return X\n",
    "    \n",
    "class TotalBaths(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Calculates total bathroom count including partial bathrooms with appropriate weighting.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[\"TotalBaths\"] = (X[\"FullBath\"] + 0.5 * X[\"HalfBath\"] + \n",
    "                           X[\"BsmtFullBath\"] + 0.5 * X[\"BsmtHalfBath\"])\n",
    "        X.drop(columns=[\"FullBath\", \"HalfBath\", \"BsmtFullBath\", \"BsmtHalfBath\"], inplace=True, errors='ignore')\n",
    "        return X\n",
    "    \n",
    "class MedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Imputes missing numerical values using median strategy for robustness against outliers.\"\"\"\n",
    "    \n",
    "    def __init__(self, columns=None, fill_value=None):\n",
    "        self.columns = columns if columns is not None else []\n",
    "        self.fill_value = fill_value\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.fill_value is None:\n",
    "            valid_columns = [col for col in self.columns if col in X.columns]\n",
    "            if valid_columns:\n",
    "                self.fill_value = X[valid_columns].median().to_dict()\n",
    "            else:\n",
    "                self.fill_value = {}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.fill_value is not None:\n",
    "            for col in self.columns:\n",
    "                if col in X.columns and col in self.fill_value:\n",
    "                    X[col] = X[col].fillna(self.fill_value[col])\n",
    "        return X\n",
    "\n",
    "class ToDataFrame(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Ensures output maintains DataFrame structure with proper column names.\n",
    "    Essential for pipeline compatibility and downstream processing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.columns_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.columns_ = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X\n",
    "        return pd.DataFrame(X, columns=self.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ea57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(numerical_values.corr(), linewidth=0.4, annot=True, fmt=\".2f\", \n",
    "            cmap=\"coolwarm\", annot_kws={\"size\": 6}, cbar_kws={\"shrink\": .9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b953dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing Pipeline {#preprocessing}\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "from pandas import DataFrame\n",
    "\n",
    "# Configuring sklearn to output pandas DataFrames for better data handling\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Defining feature imputation strategy for categorical variables\n",
    "none_fill_features: list = [\"BsmtExposure\", \"BsmtQual\", \"GarageQual\", \n",
    "                            \"GarageFinish\", \"GarageType\", \"GarageCond\", \"BsmtCond\",\n",
    "                            \"FireplaceQu\", \"MasVnrType\", \"Exterior1st\", \"BsmtFinType1\",\n",
    "                            \"BsmtFinType2\", \"Utilities\", \"Functional\", \"KitchenQual\"  ]\n",
    "\n",
    "# Features providing minimal predictive value or causing processing issues\n",
    "columns_to_drop_regardless = [\"MiscVal\", \"Alley\", \"PoolQC\", \"BsmtFinType2\", \"Fence\"]\n",
    "\n",
    "# Numerical pipeline for training data with outlier removal\n",
    "pipeline_num_train = Pipeline([\n",
    "    (\"drop_columns\", dropColumns(columns=[\"Id\"] + columns_to_drop_regardless)),\n",
    "    (\"log_transformer\", LogTransformer()),\n",
    "    (\"age_calculator\", AgeCalculator()),\n",
    "    (\"outliers_removal\", OutliersRemoval()),  # Training-specific outlier removal\n",
    "    (\"year_imputer\", YearImputer()),\n",
    "    (\"total_area\", TotalArea()),\n",
    "    (\"total_baths\", TotalBaths()),\n",
    "    (\"lot_imputer\", MedianImputer([\"LotFrontage\", \"MasVnrArea\", \n",
    "                                   \"TotalBaths\", \"BsmtFinSF1\",\n",
    "                                   \"TotalSF\", \"GarageCars\", \"BsmtUnfSF\",\n",
    "                                   \"BsmtFinSF2\", \"GarageArea\"], None)),\n",
    "    (\"standard_scaler\", StandardScaler()),\n",
    "    (\"highly_correlated_features\", HighlyCorrelatedFeatures(threshold=0.9)),\n",
    "])\n",
    "\n",
    "# Numerical pipeline for validation/test data without outlier removal\n",
    "pipeline_num_val_test = Pipeline([\n",
    "    (\"drop_columns\", dropColumns(columns=[\"Id\"] + columns_to_drop_regardless)),\n",
    "    (\"log_transformer\", LogTransformer()),\n",
    "    (\"age_calculator\", AgeCalculator()),\n",
    "    (\"year_imputer\", YearImputer()),\n",
    "    (\"total_area\", TotalArea()),\n",
    "    (\"total_baths\", TotalBaths()),\n",
    "    (\"lot_imputer\", MedianImputer([\"LotFrontage\", \"MasVnrArea\", \n",
    "                                   \"TotalBaths\", \"BsmtFinSF1\",\n",
    "                                   \"TotalSF\", \"GarageCars\", \"BsmtUnfSF\",\n",
    "                                   \"BsmtFinSF2\", \"GarageArea\"], None)),\n",
    "    (\"standard_scaler\", StandardScaler()),\n",
    "    (\"highly_correlated_features\", HighlyCorrelatedFeatures(threshold=0.9)),\n",
    "])\n",
    "\n",
    "# Categorical pipeline for training data\n",
    "pipeline_cat_train = Pipeline([\n",
    "    (\"drop_columns_cat\", dropColumns(columns=[\"Id\"] + columns_to_drop_regardless)),\n",
    "    (\"outliers_removal\", OutliersRemoval()),  # Consistent outlier handling\n",
    "    (\"feature_imputer\", FeatureImputer([], none_fill_features)),\n",
    "    (\"categorical_encoder\", CatEncoder(one_hot_features=one_hot_features, \n",
    "                                               ordinal_features_levels=ordinal_features_dict)),\n",
    "])\n",
    "\n",
    "# Categorical pipeline for validation/test data\n",
    "pipeline_cat_val_test = Pipeline([\n",
    "    (\"drop_columns_cat\", dropColumns(columns=[\"Id\"] + columns_to_drop_regardless)),\n",
    "    (\"feature_imputer\", FeatureImputer([], none_fill_features)),\n",
    "    (\"categorical_encoder\", CatEncoder(one_hot_features=one_hot_features, \n",
    "                                               ordinal_features_levels=ordinal_features_dict)),\n",
    "])  \n",
    "\n",
    "# Target variable pipeline ensuring consistent outlier removal\n",
    "y_train_pipeline = Pipeline([\n",
    "    (\"OutliersRemoval\", OutliersRemoval()),])\n",
    "\n",
    "# Extracting feature sets for pipeline processing\n",
    "X_num_train = train_data.select_dtypes(include=[np.number]).drop(columns=[\"SalePrice\"], errors='ignore').copy()\n",
    "X_cat_train = train_data.select_dtypes(include=['object']).copy()\n",
    "y_train = train_data['SalePrice'].copy()\n",
    "y_train = y_train_pipeline.fit_transform(y_train)\n",
    "\n",
    "X_cat_val = X_val.select_dtypes(include=['object']).copy()\n",
    "X_num_val= X_val.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Preserving test IDs for submission file creation\n",
    "test_ids = test_data['Id'].copy()\n",
    "X_cat_test = test_data.select_dtypes(include=['object']).copy()\n",
    "X_num_test = test_data.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Systematic pipeline execution with proper fit/transform strategy\n",
    "# Training data: fit and transform with outlier removal\n",
    "num_train = pd.DataFrame(pipeline_num_train.fit_transform(X_num_train))\n",
    "cat_train = pd.DataFrame(pipeline_cat_train.fit_transform(X_cat_train))\n",
    "\n",
    "# Fitting validation/test pipelines on training data for consistency\n",
    "pipeline_num_val_test.fit(X_num_train)\n",
    "pipeline_cat_val_test.fit(X_cat_train)\n",
    "\n",
    "# Transforming validation/test data using fitted parameters\n",
    "num_val = pd.DataFrame(pipeline_num_val_test.transform(X_num_val))\n",
    "cat_val = pd.DataFrame(pipeline_cat_val_test.transform(X_cat_val))\n",
    "num_test = pd.DataFrame(pipeline_num_val_test.transform(X_num_test))\n",
    "cat_test = pd.DataFrame(pipeline_cat_val_test.transform(X_cat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9e72a",
   "metadata": {},
   "source": [
    "### **Pipeline Strategy for Different Datasets**\n",
    "\n",
    "When handling train/validation/test data, we need different pipeline strategies:\n",
    "\n",
    "- Outlier Removal Problem\n",
    "\n",
    "- Fitting Strategy\n",
    "\n",
    "- Index Alignment\n",
    "\n",
    "- Feature Creation Order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train.isnull().sum().sort_values(ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test[\"TotalBaths\"] = num_test[\"TotalBaths\"].fillna(num_test[\"TotalBaths\"].median())\n",
    "num_test[\"TotalSF\"] = num_test[\"TotalSF\"].fillna(num_test[\"TotalSF\"].median())\n",
    "num_test[\"BsmtFinSF2\"] = num_test[\"BsmtFinSF2\"].fillna(num_test[\"BsmtFinSF2\"].median())\n",
    "num_test[\"BsmtUnfSF\"] = num_test[\"BsmtUnfSF\"].fillna(num_test[\"BsmtUnfSF\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a612dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test = cat_test.loc[:, ~cat_test.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ecb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining numerical and categorical features into unified datasets\n",
    "train_df  = num_train.join(cat_train)  \n",
    "val_df  = num_val.join(cat_val)\n",
    "test_df  = num_test.join(cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring consistent feature alignment across all datasets\n",
    "common_columns = list(set(train_df.columns) & set(val_df.columns) & set(test_df.columns))\n",
    "\n",
    "# Aligning all dataframes to common feature set\n",
    "train_df = train_df[common_columns].copy()\n",
    "val_df = val_df[common_columns].copy()\n",
    "test_df= test_df[common_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b089841",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shapes:\", train_df.shape, val_df.shape, test_df.shape)\n",
    "print(\"NaNs?  :\", train_df.isnull().any().any(),\n",
    "                  val_df.isnull().any().any(),\n",
    "                  test_df.isnull().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing any column that is in val/test but not in train: \n",
    "for col in val_df.columns:\n",
    "    if col not in train_df.columns:\n",
    "        val_df.drop(columns=col, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if val_df has some nan values\n",
    "train_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6529a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = cat_test.columns[cat_test.columns.duplicated()].unique()\n",
    "print(\"duplicate columns:\", dupes.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bebd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a432776",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad7ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Implementation {#modeling}\n",
    "\n",
    "# Establishing baseline performance with linear regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "model_LR = LinearRegression()\n",
    "model_LR.fit(train_df, y_train)\n",
    "\n",
    "# Evaluating baseline model performance\n",
    "y_pred_lr = model_LR.predict(val_df)\n",
    "rmse_lr = root_mean_squared_error(y_val, y_pred_lr)\n",
    "r2_lr = r2_score(y_val, y_pred_lr)\n",
    "\n",
    "print(f\" RMSE: {rmse_lr:,.2f}\")\n",
    "print(f\" RÂ² Score: {r2_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4a844",
   "metadata": {},
   "source": [
    "## Model Evaluation and Analysis\n",
    "\n",
    "### Ensemble Learning with Stacking Regressors\n",
    "\n",
    "The following stacking implementation goes beyond simple model averaging by learning optimal combination weights through cross-validation. The system automatically discovers which algorithms work best together and how to weight their predictions. \n",
    "\n",
    "**Current Ensemble Architecture:**\n",
    "- **5 Base Learners**: Automatically selected by Optuna from 9 algorithm options\n",
    "- **Meta-Learning Strategy**: Ridge regression learns optimal weighting of base predictions  \n",
    "- **Cross-Validation Prevention**: 5-fold CV ensures meta-learner doesn't overfit to base model outputs\n",
    "- **Algorithm Diversity**: Combines complementary approaches (linear, tree-based, kernel, instance-based)\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "**Base Model Selection Process:**\n",
    "1. *Optuna Optimization*: Binary flags determine which algorithms to include\n",
    "2. *Hyperparameter Tuning*: Each selected model gets optimized parameters\n",
    "3. *Ensemble Size Control*: Minimum 2 models required (pruning single-model trials)\n",
    "4. *Performance Evaluation*: Cross-validation scoring for robust model assessment\n",
    "\n",
    "**Current Best Combination (Post-Optimization):**\n",
    "- **Lasso Regression**: Captures linear relationships with feature selection (Î±=0.227)\n",
    "- **CatBoost***: Handles categorical features and non-linear patterns (depth=4, lr=0.073)\n",
    "- **Support Vector Regression**: Complex decision boundaries via kernel methods (C=0.764)\n",
    "- **Random Forest**: Ensemble variance reduction (170 trees, depth=20)\n",
    "- **XGBoost**: Extreme gradient boosting with regularization (170 trees, lr=0.073)\n",
    "\n",
    "**Meta-Learner Configuration:**\n",
    "- *Ridge Regression*: Optimal base model weighting (Î±=0.0025)\n",
    "- *Cross-Validation*: 5-fold CV prevents meta-learner overfitting\n",
    "- *Automatic Weight Learning*: No manual ensemble weight specification required\n",
    "\n",
    "#### Performance Evaluation \n",
    "\n",
    "**Multi-Metric Assessment:**\n",
    "- **RMSE**: Primary optimization target (root mean squared error)\n",
    "- **RÂ² Score**: Variance explanation measurement\n",
    "- **MAE**: Outlier-robust error assessment (less sensitive to extreme values)\n",
    "\n",
    "**Validation Approach:**\n",
    "- **Held-Out Validation**: 20% of training data reserved for final evaluation\n",
    "- **Outlier Preservation**: Validation/test sets retain outliers for realistic assessment\n",
    "- **Residual Analysis**: Error distribution examination revealing model behavior on extreme values\n",
    "\n",
    "This comprehensive evaluation ensures our ensemble generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ensemble learning libraries and evaluation metrics\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_squared_error as RMSE\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from sklearn.model_selection import cross_val_score as CrossValScore\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "# Core regression algorithms for ensemble construction\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import (RandomForestRegressor, StackingRegressor,\n",
    "                              GradientBoostingRegressor)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "\n",
    "# Advanced gradient boosting frameworks\n",
    "from xgboost import XGBRegressor as XGB\n",
    "from lightgbm import LGBMRegressor as GBM\n",
    "from catboost import CatBoostRegressor as CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization framework for automated model tuning\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010cf9d",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Optuna {#optimization}\n",
    "\n",
    "**What We're Doing: Automated Ensemble Discovery**\n",
    "\n",
    "Optuna handles the complex task of simultaneously optimizing two critical decisions:\n",
    "1. **Which models to include** in our stacking ensemble (9 algorithm options)\n",
    "2. **What hyperparameters** each selected model should use\n",
    "\n",
    "**The Optimization Process:**\n",
    "- **Trial-based Search**: Each trial tests a different combination of models and hyperparameters\n",
    "- **Dynamic Ensemble Composition**: Binary flags (include_lr, include_lasso, etc.) let Optuna decide which algorithms to combine\n",
    "- **Bayesian Optimization**: Optuna intelligently suggests parameter combinations based on previous trial results\n",
    "- **Cross-Validation Scoring**: Every ensemble candidate gets evaluated using 5-fold CV to prevent overfitting\n",
    "\n",
    "**Model Evaluation Strategy:**\n",
    "- **Objective Function**: Maximizes negative RMSE (converts minimization to maximization problem)\n",
    "- **Minimum Ensemble Size**: Trials with less than 2 models get pruned (stacking requires multiple base learners)\n",
    "- **Performance Tracking**: Each trial records which models were selected for later analysis\n",
    "- **Pruning Strategy**: Poor-performing trials get terminated early to save computation time\n",
    "\n",
    "**Current Implementation Logic:**\n",
    "The objective function creates a complete stacking pipeline for each trial, fits it using cross-validation, and returns the mean CV score. This approach ensures our final model selection is robust and generalizable, rather than just optimized for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f70881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial): \n",
    "    \"\"\"Optuna objective function implementing comprehensive hyperparameter optimization.\n",
    "    Simultaneously optimizes model selection and individual model parameters.\"\"\"\n",
    "    \n",
    "    # Defining hyperparameter search spaces for each algorithm\n",
    "    catboost_depth = trial.suggest_int(\"catboost_depth\", 3, 10)\n",
    "    catboost_learning_rate = trial.suggest_float(\"catboost_learning_rate\", 1e-3, 0.3, log=True)\n",
    "    catboost_iterations = trial.suggest_int(\"catboost_iterations\", 50, 500, step=50)\n",
    "    catboost_l2_leaf_reg = trial.suggest_float(\"catboost_l2_leaf_reg\", 1e-3, 10.0, log=True)\n",
    "    ridge_alpha = trial.suggest_float(\"ridge_alpha\", 1e-3, 10.0, log=True)\n",
    "    lasso_alpha = trial.suggest_float(\"lasso_alpha\", 1e-3, 10.0, log=True)\n",
    "    elastic_alpha = trial.suggest_float(\"elastic_alpha\", 1e-3, 10.0, log=True)\n",
    "    elastic_l1_ratio = trial.suggest_float(\"elastic_l1_ratio\", 0.0, 1.0)\n",
    "    svr_c = trial.suggest_float(\"svr_c\", 1e-3, 10.0, log=True)\n",
    "    svr_epsilon = trial.suggest_float(\"svr_epsilon\", 1e-3, 1.0, log=True)\n",
    "    knn_n_neighbors = trial.suggest_int(\"knn_n_neighbors\", 1, 20)\n",
    "    knn_weights = trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"])\n",
    "    knn_algorithm = trial.suggest_categorical(\"knn_algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"])\n",
    "    knn_leaf_size = trial.suggest_int(\"knn_leaf_size\", 1, 50)\n",
    "    rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 50, 300, step=10)\n",
    "    rf_max_depth = trial.suggest_int(\"rf_max_depth\", 5, 30, step=5)\n",
    "    lgb_num_leaves = trial.suggest_int(\"lgb_num_leaves\", 20, 500, step=10)\n",
    "    lgb_max_depth = trial.suggest_int(\"lgb_max_depth\", 5, 30, step=5)\n",
    "    \n",
    "    # Binary model selection flags enabling dynamic ensemble composition\n",
    "    include_lr = trial.suggest_categorical(\"include_lr\", [True, False])\n",
    "    include_lasso = trial.suggest_categorical(\"include_lasso\", [True, False])\n",
    "    include_enet = trial.suggest_categorical(\"include_enet\", [True, False])\n",
    "    include_catboost = trial.suggest_categorical(\"include_catboost\", [True, False])\n",
    "    include_svr = trial.suggest_categorical(\"include_svr\", [True, False])\n",
    "    include_knn = trial.suggest_categorical(\"include_knn\", [True, False])\n",
    "    include_rf = trial.suggest_categorical(\"include_rf\", [True, False])\n",
    "    include_lgbm = trial.suggest_categorical(\"include_lgbm\", [True, False])\n",
    "    include_xgb = trial.suggest_categorical(\"include_xgb\", [True, False])\n",
    "    \n",
    "    # Instantiating models with optimized hyperparameters\n",
    "    CatBoost = CB(depth=catboost_depth,\n",
    "                  learning_rate=catboost_learning_rate, \n",
    "                    iterations=catboost_iterations,\n",
    "                    l2_leaf_reg=catboost_l2_leaf_reg,\n",
    "                    random_state=42,)\n",
    "\n",
    "    SVR_model = SVR(C=svr_c, epsilon=svr_epsilon)\n",
    "    KNN_model = KNR(n_neighbors=knn_n_neighbors, \n",
    "                    weights=knn_weights, \n",
    "                    algorithm=knn_algorithm, \n",
    "                    leaf_size=knn_leaf_size)\n",
    "    \n",
    "    RF_model = RandomForestRegressor(n_estimators=rf_n_estimators, \n",
    "                                     max_depth=rf_max_depth, \n",
    "                                     random_state=42)\n",
    "    \n",
    "    LGBM_model = GBM(num_leaves=lgb_num_leaves,\n",
    "                    max_depth=lgb_max_depth, \n",
    "                    learning_rate=catboost_learning_rate, \n",
    "                    random_state=42)\n",
    "    \n",
    "    XGB_model = XGB(n_estimators=rf_n_estimators,\n",
    "                    max_depth=rf_max_depth, \n",
    "                    learning_rate=catboost_learning_rate,\n",
    "                    random_state=42)\n",
    "    Ridge_model = Ridge(alpha=ridge_alpha, random_state=42)\n",
    "    Lasso_model = Lasso(alpha=lasso_alpha, random_state=42)\n",
    "    ElasticNet_model = ElasticNet(alpha=elastic_alpha, \n",
    "                                 l1_ratio=elastic_l1_ratio, \n",
    "                                 random_state=42)\n",
    "    \n",
    "    # Building dynamic estimator list based on selection flags\n",
    "    estimators = []\n",
    "\n",
    "    if include_lr:\n",
    "        estimators.append((\"LR\", LinearRegression()))\n",
    "    if include_lasso:\n",
    "        estimators.append((\"Lasso\", Lasso_model))\n",
    "    if include_enet:\n",
    "        estimators.append((\"ElasticNet\", ElasticNet_model))\n",
    "    if include_catboost:\n",
    "        estimators.append((\"CatBoost\", CatBoost))\n",
    "    if include_svr:\n",
    "        estimators.append((\"SVR\", SVR_model))\n",
    "    if include_knn:\n",
    "        estimators.append((\"KNN\", KNN_model))\n",
    "    if include_rf:\n",
    "        estimators.append((\"RF\", RF_model))\n",
    "    if include_lgbm:\n",
    "        estimators.append((\"LGBM\", LGBM_model))\n",
    "    if include_xgb:\n",
    "        estimators.append((\"XGB\", XGB_model))\n",
    "        \n",
    "    # Ensuring minimum ensemble size for meaningful stacking\n",
    "    if len(estimators) < 2:\n",
    "        raise optuna.exceptions.TrialPruned() \n",
    "    meta_model = Ridge_model\n",
    "\n",
    "    # Creating stacking ensemble with dynamic composition\n",
    "    stacking_model = StackingRegressor(estimators=estimators, \n",
    "                                       final_estimator=meta_model, \n",
    "                                       cv=5,\n",
    "                                       n_jobs=-1)\n",
    "    \n",
    "    # Evaluating performance through cross-validation\n",
    "    score = CrossValScore(stacking_model, train_df, y_train,\n",
    "                          scoring = \"neg_root_mean_squared_error\",\n",
    "                          cv=KFold(n_splits=5, shuffle=True, random_state=42), n_jobs = -1) \n",
    "    \n",
    "    # Recording selected models for analysis\n",
    "    trial.set_user_attr(\"used_models\", [name for name, _ in estimators])\n",
    "    return score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd93ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = study.best_trial\n",
    "df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"user_attrs\"))\n",
    "print(\"Best model combination:\", best.user_attrs[\"used_models\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb54c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602dde48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  RMSE:\", trial.value)\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9660d",
   "metadata": {},
   "source": [
    "I used the same variable names for some parameters, some of them shar they learning rate and so ojn. If we want to be precisem we should separate each parameter for each model used, in this way they get tuned separetely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha=0.22726857509532503, random_state=42)\n",
    "catboost_model = CB(depth=4,\n",
    "                    learning_rate=0.07280162515194599, \n",
    "                    iterations=400,\n",
    "                    l2_leaf_reg=0.006096837447833604,\n",
    "                    random_state=42)\n",
    "svr_model = SVR(C=0.7635604502001735, \n",
    "                epsilon=0.28940639895970827)\n",
    "rf_model = RandomForestRegressor(n_estimators=170, \n",
    "                                  max_depth=20, \n",
    "                                  random_state=42)\n",
    "lgbm_model = GBM(num_leaves=150,\n",
    "                    max_depth=25, \n",
    "                    learning_rate=0.07280162515194599, \n",
    "                    random_state=42)\n",
    "xgb_model = XGB(n_estimators=170,\n",
    "                max_depth=170, \n",
    "                learning_rate=0.07280162515194599,\n",
    "                random_state=42)\n",
    "\n",
    "best_models_combo = [(\"Lasso\", lasso_model), (\"CatBoost\", catboost_model), (\"SVR\", svr_model), (\"RF\", rf_model), (\"XGB\", xgb_model)]\n",
    "\n",
    "final_stack = StackingRegressor(\n",
    "    estimators=best_models_combo,\n",
    "    final_estimator=Ridge(alpha=0.002527911603259286),\n",
    "    cv=5,\n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.mean())\n",
    "print(val_df.mean())\n",
    "print(y_train.mean(), y_val.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final stacking model on the training data\n",
    "final_stack.fit(train_df, y_train)\n",
    "# Predict on the validation set\n",
    "y_pred_final = final_stack.predict(val_df)\n",
    "\n",
    "# Calculate RMSE manually by taking the square root of MSE\n",
    "val_rmse = np.sqrt(RMSE(y_val, y_pred_final))\n",
    "val_r2 = R2(y_val, y_pred_final)\n",
    "val_mae = MAE(y_val, y_pred_final)\n",
    "\n",
    "print(\"Final Stacking Model Performance on Validation Set:\")\n",
    "print(f\"Validation RMSE: {val_rmse:,.2f}\")\n",
    "print(f\"Validation RÂ²: {val_r2:.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = y_val - y_pred_final\n",
    "plt.hist(errors, bins=50)\n",
    "plt.title(\"Residuals Histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad428ebf",
   "metadata": {},
   "source": [
    "Most residuals are centered around 0, suggesting the model is well-calibrated. However, there are some outliers, indicating that the model may struggle with certain data points. This is aalso suggested by MAE being very high compared to the centre of residuals' distribution. MAE is less sensitive to outliers because treats all errors equally, while MSE squares the errors, giving more weight to larger errors. \n",
    "This means that MSE is more sensitive to outliers than MAE.\n",
    "\n",
    "\n",
    "Therefore, MAE should be way less high than MSE, but in this case it is not. This suggests that the model is not performing well on some data points, which may be due to outliers or other factors. In fact, we didn't remove outliers from validation set and test, on porpuse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea3e2bf",
   "metadata": {},
   "source": [
    "#### Feature Inspection and Importance Analysis\n",
    "\n",
    "Feature importance analysis provides insights into which features contribute most to model predictions. This helps identify key drivers of house prices and can guide future feature engineering efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features = rf_model.feature_importances_\n",
    "lgb_features = lgbm_model.feature_importances_\n",
    "xgb_features = xgb_model.feature_importances_\n",
    "catboost_features = catboost_model.feature_importances_\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": train_df.columns,\n",
    "    \"RF_Importance\": rf_features,\n",
    "    \"LGBM_Importance\": lgb_features,\n",
    "    \"XGB_Importance\": xgb_features,\n",
    "    \"CatBoost_Importance\": catboost_features\n",
    "})\n",
    "\n",
    "feature_importances = feature_importances.set_index(\"Feature\")\n",
    "feature_importances = feature_importances.sort_values(by=\"RF_Importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "feature_importances.plot(kind=\"bar\", figsize=(15, 10), fontsize=10)\n",
    "plt.title(\"Feature Importances from Different Models\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835f633",
   "metadata": {},
   "source": [
    "### Prediction on Test set and creation of submission file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = final_stack.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse = np.sqrt(RMSE(y_val, y_pred_final))\n",
    "test_r2 = R2(y_val, y_pred_final)\n",
    "test_mae = MAE(y_val, y_pred_final)\n",
    "\n",
    "print(\"Final Stacking Model Performance on Validation Set:\")\n",
    "print(f\"Validation RMSE: {test_rmse:,.2f}\")\n",
    "print(f\"Validation RÂ²: {test_r2:.4f}\")\n",
    "print(f\"Validation MAE: {test_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"Id\": test_ids,\n",
    "                           \"SalePrice\": y_test_pred})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_VU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
